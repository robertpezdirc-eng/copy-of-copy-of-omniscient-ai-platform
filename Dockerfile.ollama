# Dockerfile for Ollama on Google Cloud Run
# This image contains Ollama LLM server optimized for IIoT data analysis

# Use official Ollama image as base
FROM ollama/ollama:latest

# Set environment variables
# OLLAMA_MODELS defines which model to preload at build time
ENV OLLAMA_MODELS=llama3

# Install the model at build time so it's included in the image
# This reduces cold start time in Cloud Run
RUN ollama pull ${OLLAMA_MODELS}

# Optionally pull additional models for specific use cases
# Uncomment to include more models:
# RUN ollama pull gemma:2b
# RUN ollama pull mistral:7b

# Ollama runs on port 11434 by default
# Cloud Run will handle port exposure
EXPOSE 11434

# Health check for container orchestration
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:11434/api/tags || exit 1

# Start Ollama server
# The serve command starts the API server that listens for requests
CMD ["ollama", "serve"]
