# Dockerfile for Ollama LLM Service
# Uses official Ollama image and pre-loads the llama3 model
FROM ollama/ollama:latest

# Set the default LLM model to be downloaded and loaded at startup
# Change "llama3" to the model you want to use (e.g., "gemma:2b")
ENV OLLAMA_MODELS=llama3

# Install the model during build so it's included in the image
# This reduces startup time in Cloud Run
RUN ollama pull ${OLLAMA_MODELS}

# Ollama runs on port 11434 by default
# Cloud Run will handle exposing this port
EXPOSE 11434

# Start the Ollama server
CMD ["ollama", "serve"]
