# Dockerfile for Ollama LLM Service
# Uses official Ollama image and pre-loads the llama3 model
FROM ollama/ollama:latest

# Set the default LLM model to be downloaded and loaded at startup
# Change "llama3" to the model you want to use (e.g., "gemma:2b")
ENV OLLAMA_MODELS=llama3

# Pre-download the model during build to reduce startup time
# Start Ollama server in background, pull model, then stop
RUN ollama serve & sleep 5 && ollama pull ${OLLAMA_MODELS} && pkill ollama

# Ollama runs on port 11434 by default
# Cloud Run will handle exposing this port
EXPOSE 11434

# Start the Ollama server
CMD ["ollama", "serve"]
