# Cloud Build Configuration for Ollama LLM Service
# Builds and deploys the Ollama service to Cloud Run

steps:
  # Build the Ollama Docker image
  - name: 'gcr.io/cloud-builders/docker'
    args:
      - 'build'
      - '-t'
      - 'gcr.io/${PROJECT_ID}/ollama-llm-service:${SHORT_SHA}'
      - '-t'
      - 'gcr.io/${PROJECT_ID}/ollama-llm-service:latest'
      - '-f'
      - 'Dockerfile.ollama'
      - '.'
    id: 'build-ollama'

  # Push the Ollama image to Container Registry
  - name: 'gcr.io/cloud-builders/docker'
    args:
      - 'push'
      - 'gcr.io/${PROJECT_ID}/ollama-llm-service:${SHORT_SHA}'
    id: 'push-ollama'
    waitFor: ['build-ollama']

  # Deploy Ollama service to Cloud Run
  - name: 'gcr.io/cloud-builders/gcloud'
    args:
      - 'run'
      - 'deploy'
      - 'ollama-ai-inference'
      - '--image=gcr.io/${PROJECT_ID}/ollama-llm-service:${SHORT_SHA}'
      - '--platform=managed'
      - '--region=${_REGION}'
      - '--cpu=4'
      - '--memory=8Gi'
      - '--no-allow-unauthenticated'
      - '--max-instances=10'
      - '--min-instances=0'
      - '--ingress=all'
      - '--port=11434'
      - '--set-env-vars=OLLAMA_MODELS=llama3'
    id: 'deploy-ollama'
    waitFor: ['push-ollama']

substitutions:
  _REGION: 'europe-west1'

options:
  machineType: 'E2_HIGHCPU_8'
  logging: CLOUD_LOGGING_ONLY

timeout: '1800s'  # 30 minutes for model download
